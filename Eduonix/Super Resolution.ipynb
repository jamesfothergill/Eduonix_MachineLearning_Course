{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba337ec-8afb-4a1a-9352-37a957e30ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]\n",
      "keras: 3.3.3\n",
      "cv2: 4.9.0\n",
      "numpy: 1.26.3\n",
      "matplotlib: 3.8.2\n",
      "skimage: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import keras\n",
    "import cv2\n",
    "import numpy\n",
    "import matplotlib\n",
    "import skimage\n",
    "\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('keras: {}'.format(keras.__version__))\n",
    "print('cv2: {}'.format(cv2.__version__))\n",
    "print('numpy: {}'.format(numpy.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('skimage: {}'.format(skimage.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c023c510-9224-47ac-9707-33cd9ec7148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "# python magic function (makes graphs/images plot in notebook and not pop up window)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0efa41d-88af-48f9-aee3-021f85e363b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to peak signal to noise ratio PNSR ( had to add channel_axis=-1)\n",
    "def psnr(target, ref):\n",
    "\n",
    "    # assume RGB image\n",
    "    target_data = target.astype(float)\n",
    "    ref_data = ref.astype(float)\n",
    "\n",
    "    diff = ref_data - target_data\n",
    "    diff = diff.flatten('C')\n",
    "\n",
    "    rmse = math.sqrt(np.mean(diff ** 2.))\n",
    "\n",
    "    return 20 * math.log10(255. / rmse)\n",
    "\n",
    "# define function for mean squared error (MSE)\n",
    "def mse(target, ref):\n",
    "    # the MSE between the two images is the sum of the squared difference between the two images\n",
    "    err = np.sum((target.astype('float') - ref.astype('float')) ** 2)\n",
    "    err /= float(target.shape[0] * target.shape[1])\n",
    "\n",
    "    return err\n",
    "\n",
    "# define function that combines all three image quality metrics\n",
    "def compare_images(target, ref):\n",
    "    scores = []\n",
    "    scores.append(psnr(target, ref))\n",
    "    scores.append(mse(target, ref))\n",
    "    scores.append(ssim(target, ref, channel_axis=-1, multichannel =True))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce77cf-b396-407c-89fe-e9b28fccc094",
   "metadata": {},
   "source": [
    "https://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53908f3-4f2f-4c24-b638-fc43aeefc0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving baboon.bmp\n",
      "Saving baby_GT.bmp\n",
      "Saving barbara.bmp\n",
      "Saving bird_GT.bmp\n",
      "Saving butterfly_GT.bmp\n",
      "Saving coastguard.bmp\n",
      "Saving comic.bmp\n",
      "Saving face.bmp\n",
      "Saving flowers.bmp\n",
      "Saving foreman.bmp\n",
      "Saving head_GT.bmp\n",
      "Saving lenna.bmp\n",
      "Saving monarch.bmp\n",
      "Saving pepper.bmp\n",
      "Saving ppt3.bmp\n",
      "Saving woman_GT.bmp\n",
      "Saving zebra.bmp\n"
     ]
    }
   ],
   "source": [
    "# Preparing Images (degraded) by resizing\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "def prepare_images(path, factor):\n",
    "\n",
    "    # loop through the files in the directory\n",
    "    for file in os.listdir(path):\n",
    "        try:\n",
    "          # open the file\n",
    "          img = cv2.imread(path + '/' + file)\n",
    "\n",
    "          # find old and new image dimensions\n",
    "          h, w, _ = img.shape\n",
    "          new_height = int(h / factor)\n",
    "          new_width = int(w / factor)\n",
    "\n",
    "          # resize the image - down\n",
    "          img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "          # resize the image - up\n",
    "          img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "          # save the image\n",
    "          print('Saving {}'.format(file))\n",
    "          cv2.imwrite('images/{}'.format(file), img)\n",
    "        except:\n",
    "          print('ERROR for file-', file, '!')\n",
    "          pass\n",
    "\n",
    "prepare_images('source/', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0d86d2-d883-40c8-b2df-01576f697826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baboon.bmp\n",
      "PSNR: 22.157084083442548\n",
      "MSE: 1187.1161333333334\n",
      "SSIM: 0.629277587900277\n",
      "\n",
      "baby_GT.bmp\n",
      "PSNR: 34.37180640966199\n",
      "MSE: 71.28874588012695\n",
      "SSIM: 0.9356987872724932\n",
      "\n",
      "barbara.bmp\n",
      "PSNR: 25.906629837568126\n",
      "MSE: 500.65508535879627\n",
      "SSIM: 0.8098632646406401\n",
      "\n",
      "bird_GT.bmp\n",
      "PSNR: 32.896644728720005\n",
      "MSE: 100.12375819830247\n",
      "SSIM: 0.9533644866026473\n",
      "\n",
      "butterfly_GT.bmp\n",
      "PSNR: 24.782076560337416\n",
      "MSE: 648.6254119873047\n",
      "SSIM: 0.8791344763843051\n",
      "\n",
      "coastguard.bmp\n",
      "PSNR: 27.161600663887082\n",
      "MSE: 375.00887784090907\n",
      "SSIM: 0.756950063354931\n",
      "\n",
      "comic.bmp\n",
      "PSNR: 23.799861502225532\n",
      "MSE: 813.2338836565096\n",
      "SSIM: 0.8347335416398209\n",
      "\n",
      "face.bmp\n",
      "PSNR: 30.99220650287191\n",
      "MSE: 155.23189718546524\n",
      "SSIM: 0.8008439492289884\n",
      "\n",
      "flowers.bmp\n",
      "PSNR: 27.454504805386147\n",
      "MSE: 350.55093922651935\n",
      "SSIM: 0.8697286286974628\n",
      "\n",
      "foreman.bmp\n",
      "PSNR: 30.14456532664372\n",
      "MSE: 188.6883483270202\n",
      "SSIM: 0.933268417388899\n",
      "\n",
      "head_GT.bmp\n",
      "PSNR: 31.020502848237534\n",
      "MSE: 154.2237755102041\n",
      "SSIM: 0.8011121330733371\n",
      "\n",
      "lenna.bmp\n",
      "PSNR: 31.47349297867539\n",
      "MSE: 138.94800567626953\n",
      "SSIM: 0.8460989200521499\n",
      "\n",
      "monarch.bmp\n",
      "PSNR: 30.196242365288896\n",
      "MSE: 186.45643615722656\n",
      "SSIM: 0.9439574293434104\n",
      "\n",
      "pepper.bmp\n",
      "PSNR: 29.88947161686106\n",
      "MSE: 200.1033935546875\n",
      "SSIM: 0.8357937568464359\n",
      "\n",
      "ppt3.bmp\n",
      "PSNR: 24.84926168950471\n",
      "MSE: 638.6684263912582\n",
      "SSIM: 0.9284023942315316\n",
      "\n",
      "woman_GT.bmp\n",
      "PSNR: 29.326236280817465\n",
      "MSE: 227.812729498164\n",
      "SSIM: 0.9335397280466592\n",
      "\n",
      "zebra.bmp\n",
      "PSNR: 27.909840639329513\n",
      "MSE: 315.6585459528818\n",
      "SSIM: 0.8911656209329116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing the generated images using the image quality metrics\n",
    "\n",
    "for file in os.listdir('images/'):\n",
    "    \n",
    "      # open target and reference images\n",
    "      target = cv2.imread('images/{}'.format(file))\n",
    "      ref = cv2.imread('source/{}'.format(file))\n",
    "\n",
    "      # calculate score\n",
    "      scores = compare_images(target, ref)\n",
    "\n",
    "      # print all three scores with new line characters (\\n)\n",
    "      print('{}\\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(file, scores[0], scores[1], scores[2]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573f19e4-48a4-4e3f-854e-e064d98714f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SRCNN Model\n",
    "\n",
    "def model():\n",
    "\n",
    "    # define model type\n",
    "    SRCNN = Sequential()\n",
    "\n",
    "    # add model layers\n",
    "    SRCNN.add(Conv2D(filters=128, kernel_size = (9, 9), kernel_initializer='glorot_uniform',\n",
    "                     activation='relu', padding='valid', use_bias=True, input_shape=(None, None, 1)))\n",
    "    SRCNN.add(Conv2D(filters=64, kernel_size = (3, 3), kernel_initializer='glorot_uniform',\n",
    "                     activation='relu', padding='same', use_bias=True))\n",
    "    SRCNN.add(Conv2D(filters=1, kernel_size = (5, 5), kernel_initializer='glorot_uniform',\n",
    "                     activation='linear', padding='valid', use_bias=True))\n",
    "\n",
    "    # define optimizer\n",
    "    adam = Adam(lr=0.0003)\n",
    "\n",
    "    # compile model\n",
    "    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "    return SRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb033a-df0b-4424-9664-4cf55944e72a",
   "metadata": {},
   "source": [
    "https://github.com/MarkPrecursor/SRCNN-keras\n",
    "\n",
    "pre trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc66200-70a9-4c24-b306-57b20ef65f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define necessary image processing functions\n",
    "\n",
    "def modcrop(img, scale):\n",
    "    tmpsz = img.shape\n",
    "    sz = tmpsz[0:2]\n",
    "    sz = sz - np.mod(sz, scale)\n",
    "    img = img[0:sz[0], 1:sz[1]]\n",
    "    return img\n",
    "\n",
    "def shave(image, border):\n",
    "    img = image[boarder: -boader, boader: - boader]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674652eb-9e5d-47d9-ac5b-df61fe350925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definine main prediction function\n",
    "\n",
    "def predict(image_path):\n",
    "\n",
    "    # load the srcnn model with weights\n",
    "    srcnn = model()\n",
    "    srcnn.load_weights('3051crop_weight_200.h5')\n",
    "\n",
    "    # load the degraded and reference images\n",
    "    path, file = os.path.split(image_path)\n",
    "    degraded = cv2.imread(image_path)\n",
    "    ref = cv2.imread('source/{}'.format(file))\n",
    "\n",
    "    # preprocess the image with modcrop\n",
    "    ref = modcrop(ref, 3)\n",
    "    degraded = modcrop(degraded, 3)\n",
    "\n",
    "    # convert the image to YCrCb - (srcnn trained on Y channel)\n",
    "    temp = cv2.cvtColor(degraded, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # create image slice and normalise\n",
    "    Y = np.zeros((1, temp.shape[0], temp.shape[1], 1), dtype=float)\n",
    "    Y[0, :, :, 0] = temp[:, :, 0].astype(float) / 255\n",
    "\n",
    "    # perform super-resolution with srcnn\n",
    "    pre = srcnn.predict(Y, batch_size = 1)\n",
    "\n",
    "    #post-process output\n",
    "    pre *= 255\n",
    "    pre[pre[:] > 255] =255\n",
    "    pre[pre[:] < 0] = 0\n",
    "    pre = pre.astype(np.uint8)\n",
    "\n",
    "    # copy Y channel back to image and convert to BGR\n",
    "    temp = shave(temp, 6)\n",
    "    temp[:, :, 0] = pre[0, :, :, 0]\n",
    "    output =cv2.cvtColor(temp, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "    # remove border from reference and degraded image\n",
    "    ref = shave(ref.astype(np.uint8), 6)\n",
    "    degraded = shave(degraded.astype(np.uint8), 6)\n",
    "\n",
    "    # image quality calculations\n",
    "    scores = []\n",
    "    scores.append(compare_images(degraded, ref))\n",
    "    scores.append(compare_images(output, ref))\n",
    "\n",
    "    # returnimages and scores\n",
    "    return ref, degraded, output, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d81152b-6e86-41d9-be8d-090b694b918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v8255\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.0003}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ref, degraded, output, scores \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages/flowers.bmp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#print all scores for all images\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDegraded Image: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPSNR: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSSIM: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(image_path):\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# load the srcnn model with weights\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     srcnn \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     srcnn\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3051crop_weight_200.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# load the degraded and reference images\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m SRCNN\u001b[38;5;241m.\u001b[39madd(Conv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglorot_uniform\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m                  activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# define optimizer\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m adam \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# compile model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m SRCNN\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39madam, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:62\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     61\u001b[0m ):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:22\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:37\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.0003}"
     ]
    }
   ],
   "source": [
    "ref, degraded, output, scores = predict('images/flowers.bmp')\n",
    "\n",
    "#print all scores for all images\n",
    "print('Degraded Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(scores[0][0], scores[0][1], scores[0][2]))\n",
    "print('Reconstructed Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(scores[1][0], scores[1][1], scores[1][2]))\n",
    "\n",
    "# display images as subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize = (20, 8))\n",
    "axs[0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Original')\n",
    "axs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('Degraded')\n",
    "axs[2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "axs[2].set_title('SRCNN')\n",
    "\n",
    "# remove the x and y tick marks\n",
    "for ax in axs:\n",
    "    ax,set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c4924-1af1-44ac-b5c4-d30709aee465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
